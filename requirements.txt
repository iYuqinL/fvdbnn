torch>=2.7.1
fvdb-core>=0.3.1,<0.4.0a
flash_attn @ git+https://github.com/Dao-AILab/flash-attention.git@v2.8.0.post2
