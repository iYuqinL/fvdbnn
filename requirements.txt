torch>=2.7.1
fvdb-core>=0.3.0,<0.4.0
flash_attn @ git+https://github.com/Dao-AILab/flash-attention.git@v2.8.0.post2
